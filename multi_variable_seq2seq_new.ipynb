{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用之前5天的数据，对之后48小时的空气质量进行预测，模型如下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://p3rz3gu1u.bkt.clouddn.com/2018-04-19-seq2seq_model.png)\n",
    "<caption><center> **Figure 1**: lstm model</center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvdev/tf/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "from utils.plot_util import plot_forecast_and_actual_example\n",
    "from metrics.metrics import SMAPE_on_dataset_v1\n",
    "from seq2seq.seq2seq_data_util import get_training_statistics, generate_training_set, generate_dev_set\n",
    "from seq2seq.multi_variable_seq2seq_model_parameters import build_graph\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "gpu_config = tf.ConfigProto()\n",
    "gpu_config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=gpu_config)\n",
    "KTF.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多变量版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_list = ['dongsi_aq','tiantan_aq','guanyuan_aq','wanshouxigong_aq','aotizhongxin_aq',\n",
    "            'nongzhanguan_aq','wanliu_aq','beibuxinqu_aq','zhiwuyuan_aq','fengtaihuayuan_aq',\n",
    "            'yungang_aq','gucheng_aq','fangshan_aq','daxing_aq','yizhuang_aq','tongzhou_aq',\n",
    "            'shunyi_aq','pingchang_aq','mentougou_aq','pinggu_aq','huairou_aq','miyun_aq',\n",
    "            'yanqin_aq','dingling_aq','badaling_aq','miyunshuiku_aq','donggaocun_aq',\n",
    "            'yongledian_aq','yufa_aq','liulihe_aq','qianmen_aq','yongdingmennei_aq',\n",
    "            'xizhimenbei_aq','nansanhuan_aq','dongsihuan_aq']            \n",
    "X_aq_list = [\"PM2.5\",\"PM10\",\"O3\",\"CO\",\"SO2\",\"NO2\"]  \n",
    "y_aq_list = [\"PM2.5\",\"PM10\",\"O3\"]\n",
    "X_meo_list = [\"temperature\",\"pressure\",\"humidity\",\"direction\",\"speed/kph\"]\n",
    "use_day=True\n",
    "pre_days=5\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "少变量版本（测试）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# station_list = ['aotizhongxin_aq']            \n",
    "# X_aq_list = [\"PM2.5\",\"PM10\",\"O3\",\"CO\",\"SO2\",\"NO2\"]  \n",
    "# y_aq_list = [\"PM2.5\"]\n",
    "# X_meo_list = [\"temperature\",\"pressure\",\"humidity\",\"direction\",\"speed/kph\"]\n",
    "# use_day = True\n",
    "# pre_days = 5\n",
    "# batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare test datasets in 3-D format - (batch_size, time_step, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = generate_dev_set(station_list=station_list,\n",
    "                                  X_aq_list=X_aq_list, \n",
    "                                  y_aq_list=y_aq_list, \n",
    "                                  X_meo_list=None,\n",
    "                                  pre_days=pre_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 120, 210) (17, 48, 105)\n"
     ]
    }
   ],
   "source": [
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试生成一个训练样本，确保是我们想要的尺寸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 120, 210) (128, 48, 105)\n"
     ]
    }
   ],
   "source": [
    "X_training_batch, y_training_batch = generate_training_set(station_list=station_list,\n",
    "                                                           X_aq_list=X_aq_list,\n",
    "                                                           y_aq_list=y_aq_list,\n",
    "                                                           pre_days=pre_days,\n",
    "                                                           X_meo_list=None,\n",
    "                                                           use_day=True,\n",
    "                                                           batch_size=batch_size)\n",
    "print(X_training_batch.shape, y_training_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build the model and train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = pre_days * 24\n",
    "output_seq_len = 48\n",
    "hidden_dim = 512\n",
    "input_dim = 210\n",
    "output_dim = 105\n",
    "num_stacked_layers = 3\n",
    "\n",
    "learning_rate=1e-3\n",
    "lambda_l2_reg=0.003\n",
    "GRADIENT_CLIPPING=2.5\n",
    "total_iteractions = 2000\n",
    "KEEP_RATE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = build_graph(feed_previous=False, \n",
    "                        input_seq_len=input_seq_len, \n",
    "                        output_seq_len=output_seq_len, \n",
    "                        hidden_dim=hidden_dim, \n",
    "                        input_dim=input_dim, \n",
    "                        output_dim=output_dim, \n",
    "                        num_stacked_layers=num_stacked_layers, \n",
    "                        learning_rate=learning_rate,\n",
    "                        lambda_l2_reg=lambda_l2_reg,\n",
    "                        GRADIENT_CLIPPING=GRADIENT_CLIPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training losses: \n",
      "loss after 0/2000 iteractions : 151.999\n",
      "loss after 10/2000 iteractions : 79.616\n",
      "loss after 20/2000 iteractions : 74.756\n",
      "loss after 30/2000 iteractions : 68.966\n",
      "loss after 40/2000 iteractions : 65.922\n",
      "loss after 50/2000 iteractions : 64.076\n",
      "loss after 60/2000 iteractions : 62.529\n",
      "loss after 70/2000 iteractions : 60.709\n",
      "loss after 80/2000 iteractions : 59.410\n",
      "loss after 90/2000 iteractions : 57.951\n",
      "loss after 100/2000 iteractions : 56.729\n",
      "loss after 110/2000 iteractions : 55.182\n",
      "loss after 120/2000 iteractions : 53.705\n",
      "loss after 130/2000 iteractions : 52.419\n",
      "loss after 140/2000 iteractions : 50.912\n",
      "loss after 150/2000 iteractions : 49.722\n",
      "loss after 160/2000 iteractions : 47.996\n",
      "loss after 170/2000 iteractions : 47.406\n",
      "loss after 180/2000 iteractions : 46.253\n",
      "loss after 190/2000 iteractions : 45.130\n",
      "loss after 200/2000 iteractions : 44.166\n",
      "loss after 210/2000 iteractions : 42.739\n",
      "loss after 220/2000 iteractions : 41.732\n",
      "loss after 230/2000 iteractions : 40.680\n",
      "loss after 240/2000 iteractions : 39.481\n",
      "loss after 250/2000 iteractions : 38.516\n",
      "loss after 260/2000 iteractions : 37.204\n",
      "loss after 270/2000 iteractions : 36.216\n",
      "loss after 280/2000 iteractions : 35.391\n",
      "loss after 290/2000 iteractions : 34.607\n",
      "loss after 300/2000 iteractions : 33.467\n",
      "loss after 310/2000 iteractions : 32.861\n",
      "loss after 320/2000 iteractions : 31.781\n",
      "loss after 330/2000 iteractions : 31.029\n",
      "loss after 340/2000 iteractions : 29.731\n",
      "loss after 350/2000 iteractions : 29.219\n",
      "loss after 360/2000 iteractions : 27.894\n",
      "loss after 370/2000 iteractions : 27.121\n",
      "loss after 380/2000 iteractions : 26.267\n",
      "loss after 390/2000 iteractions : 25.512\n",
      "loss after 400/2000 iteractions : 24.541\n",
      "loss after 410/2000 iteractions : 23.742\n",
      "loss after 420/2000 iteractions : 23.157\n",
      "loss after 430/2000 iteractions : 22.567\n",
      "loss after 440/2000 iteractions : 21.916\n",
      "loss after 450/2000 iteractions : 21.038\n",
      "loss after 460/2000 iteractions : 20.593\n",
      "loss after 470/2000 iteractions : 19.801\n",
      "loss after 480/2000 iteractions : 19.250\n",
      "loss after 490/2000 iteractions : 18.449\n",
      "loss after 500/2000 iteractions : 17.814\n",
      "loss after 510/2000 iteractions : 17.203\n",
      "loss after 520/2000 iteractions : 16.581\n",
      "loss after 530/2000 iteractions : 16.042\n",
      "loss after 540/2000 iteractions : 15.392\n",
      "loss after 550/2000 iteractions : 14.880\n",
      "loss after 560/2000 iteractions : 14.397\n",
      "loss after 570/2000 iteractions : 13.952\n",
      "loss after 580/2000 iteractions : 13.442\n",
      "loss after 590/2000 iteractions : 12.970\n",
      "loss after 600/2000 iteractions : 12.599\n",
      "loss after 610/2000 iteractions : 12.174\n",
      "loss after 620/2000 iteractions : 11.683\n",
      "loss after 630/2000 iteractions : 11.368\n",
      "loss after 640/2000 iteractions : 10.938\n",
      "loss after 650/2000 iteractions : 10.576\n",
      "loss after 660/2000 iteractions : 10.277\n",
      "loss after 670/2000 iteractions : 9.882\n",
      "loss after 680/2000 iteractions : 9.612\n",
      "loss after 690/2000 iteractions : 9.339\n",
      "loss after 700/2000 iteractions : 9.145\n",
      "loss after 710/2000 iteractions : 9.058\n",
      "loss after 720/2000 iteractions : 8.644\n",
      "loss after 730/2000 iteractions : 8.403\n",
      "loss after 740/2000 iteractions : 8.138\n",
      "loss after 750/2000 iteractions : 7.887\n",
      "loss after 760/2000 iteractions : 7.712\n",
      "loss after 770/2000 iteractions : 7.485\n",
      "loss after 780/2000 iteractions : 7.140\n",
      "loss after 790/2000 iteractions : 6.944\n",
      "loss after 800/2000 iteractions : 6.736\n",
      "loss after 810/2000 iteractions : 6.523\n",
      "loss after 820/2000 iteractions : 6.292\n",
      "loss after 830/2000 iteractions : 6.115\n",
      "loss after 840/2000 iteractions : 5.937\n",
      "loss after 850/2000 iteractions : 5.751\n",
      "loss after 860/2000 iteractions : 5.600\n",
      "loss after 870/2000 iteractions : 5.443\n",
      "loss after 880/2000 iteractions : 5.276\n",
      "loss after 890/2000 iteractions : 5.103\n",
      "loss after 900/2000 iteractions : 5.006\n",
      "loss after 910/2000 iteractions : 4.868\n",
      "loss after 920/2000 iteractions : 4.812\n",
      "loss after 930/2000 iteractions : 4.729\n",
      "loss after 940/2000 iteractions : 4.577\n",
      "loss after 950/2000 iteractions : 4.348\n",
      "loss after 960/2000 iteractions : 4.256\n",
      "loss after 970/2000 iteractions : 4.114\n",
      "loss after 980/2000 iteractions : 4.010\n",
      "loss after 990/2000 iteractions : 3.903\n",
      "loss after 1000/2000 iteractions : 3.748\n",
      "loss after 1010/2000 iteractions : 3.645\n",
      "loss after 1020/2000 iteractions : 3.538\n",
      "loss after 1030/2000 iteractions : 3.425\n",
      "loss after 1040/2000 iteractions : 3.337\n",
      "loss after 1050/2000 iteractions : 3.274\n",
      "loss after 1060/2000 iteractions : 3.213\n",
      "loss after 1070/2000 iteractions : 3.095\n",
      "loss after 1080/2000 iteractions : 3.020\n",
      "loss after 1090/2000 iteractions : 2.959\n",
      "loss after 1100/2000 iteractions : 2.879\n",
      "loss after 1110/2000 iteractions : 2.805\n",
      "loss after 1120/2000 iteractions : 2.733\n",
      "loss after 1130/2000 iteractions : 2.716\n",
      "loss after 1140/2000 iteractions : 2.614\n",
      "loss after 1150/2000 iteractions : 2.577\n",
      "loss after 1160/2000 iteractions : 2.534\n",
      "loss after 1170/2000 iteractions : 2.421\n",
      "loss after 1180/2000 iteractions : 2.409\n",
      "loss after 1190/2000 iteractions : 2.425\n",
      "loss after 1200/2000 iteractions : 2.350\n",
      "loss after 1210/2000 iteractions : 2.235\n",
      "loss after 1220/2000 iteractions : 2.204\n",
      "loss after 1230/2000 iteractions : 2.128\n",
      "loss after 1240/2000 iteractions : 2.030\n",
      "loss after 1250/2000 iteractions : 1.962\n",
      "loss after 1260/2000 iteractions : 1.885\n",
      "loss after 1270/2000 iteractions : 1.846\n",
      "loss after 1280/2000 iteractions : 1.798\n",
      "loss after 1290/2000 iteractions : 1.738\n",
      "loss after 1300/2000 iteractions : 1.691\n",
      "loss after 1310/2000 iteractions : 1.649\n",
      "loss after 1320/2000 iteractions : 1.615\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "    losses = []\n",
    "    print(\"Training losses: \")\n",
    "    for i in range(total_iteractions):\n",
    "        batch_input, batch_output = generate_training_set(station_list,\n",
    "                                                          X_aq_list,\n",
    "                                                          y_aq_list,\n",
    "                                                          X_meo_list=None,\n",
    "                                                          use_day=use_day,\n",
    "                                                          pre_days=pre_days,\n",
    "                                                          batch_size=batch_size)\n",
    "\n",
    "        \n",
    "        feed_dict = {rnn_model['enc_inp'][t]: batch_input[:,t,:] for t in range(input_seq_len)}\n",
    "        feed_dict.update({rnn_model['target_seq'][t]: batch_output[:,t,:] for t in range(output_seq_len)})\n",
    "        _, loss_t = sess.run([rnn_model['train_op'], rnn_model['loss']], feed_dict) \n",
    "        \n",
    "        if i%10 == 0:\n",
    "            print(\"loss after %d/%d iteractions : %.3f\" %(i, total_iteractions, loss_t))\n",
    "            \n",
    "            # 想要对训练过程中训练集的 smape 进行监督，发现模型并不是处在“预测”的状态，因此放弃\n",
    "            # train_preds = sess.run(rnn_model['reshaped_outputs'], feed_dict)\n",
    "            # train_preds = [np.expand_dims(pred, 1) for pred in train_preds]\n",
    "            # train_preds = np.concatenate(train_preds, axis = 1)\n",
    "            \n",
    "        losses.append(loss_t)\n",
    "        \n",
    "    temp_saver = rnn_model['saver']()\n",
    "    save_path = temp_saver.save(sess, os.path.join('./seq2seq/new_multi_variable_model_results/', 'multivariate_ts_pollution_case'))\n",
    "        \n",
    "print(\"Checkpoint saved at: \", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on test \n",
    "Notice the batch prediction which is different to previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = build_graph(feed_previous=True, \n",
    "                        input_seq_len=input_seq_len, \n",
    "                        output_seq_len=output_seq_len, \n",
    "                        hidden_dim=hidden_dim, \n",
    "                        input_dim=input_dim, \n",
    "                        output_dim=output_dim, \n",
    "                        num_stacked_layers=num_stacked_layers, \n",
    "                        learning_rate=learning_rate,\n",
    "                        lambda_l2_reg=lambda_l2_reg,\n",
    "                        GRADIENT_CLIPPING=GRADIENT_CLIPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver = rnn_model['saver']().restore(sess,  os.path.join('./seq2seq/new_multi_variable_model_results/', 'multivariate_ts_pollution_case'))\n",
    "    \n",
    "    feed_dict = {rnn_model['enc_inp'][t]: test_x[:, t, :] for t in range(input_seq_len)} # batch prediction\n",
    "    feed_dict.update({rnn_model['target_seq'][t]: np.zeros([test_x.shape[0], output_dim], dtype=np.float32) for t in range(output_seq_len)})\n",
    "    final_preds = sess.run(rnn_model['reshaped_outputs'], feed_dict)\n",
    "    \n",
    "    final_preds = [np.expand_dims(pred, 1) for pred in final_preds]\n",
    "    final_preds = np.concatenate(final_preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of predictions is \",final_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of many featutres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_features = []\n",
    "for station in station_list : \n",
    "    for aq_feature in y_aq_list :\n",
    "        output_features.append(station + \"_\" + aq_feature)\n",
    "\n",
    "# 特征要和训练时候的特征顺序保持一致\n",
    "output_features.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_features)\n",
    "print(\"Number of features is : \", len(output_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 预测值普遍在 O3 上表现较好，另外两个参数　PM2.5 和　PM10 上通常捕捉不到高频分量\n",
    "for i in range(len(output_features)):\n",
    "    plot_forecast_and_actual_example(test_x, test_y, final_preds, output_features, index=0, feature_index=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 某个特征在整个dev数据集时间跨度上的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_index = 0\n",
    "test_y_expand = np.concatenate([test_y[i,:,feature_index] for i in range(0, test_y.shape[0])], axis = 0)\n",
    "final_preds_expand = np.concatenate([final_preds[i,:,feature_index] for i in range(0, final_preds.shape[0])], axis = 0)\n",
    "plt.plot(final_preds_expand, color = 'orange', label = 'predicted')\n",
    "plt.plot(test_y_expand, color = 'blue', label = 'actual')\n",
    "plt.title(\"test data - one month\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smapes of all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入训练样本的统计量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = get_training_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算 smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aver_smapes, smapes_of_features = SMAPE_on_dataset_v1(test_y, final_preds, output_features, statistics, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smape value on all features\n",
    "smapes_of_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The average smape on all features in the dev set is : \",aver_smapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- epoch - aver_smape relationship\n",
    "    - 50 0.85\n",
    "    - 1000 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ChangeLog\n",
    "- 0427 v0\n",
    "    - 完成了第一版本模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
